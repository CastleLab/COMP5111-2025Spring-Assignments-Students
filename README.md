# HKUST COMP5111 (2025Spring) Assignment 1

## Deadline: 11:55 pm, 15 March 2025

## Update:
(Jan 26): Release the [tutorial workshop slide (preview version)](tutorials/COMP5111_25S_workshop_songqiang_jan26previewver.pdf). *You are highly recommended to preview the slide (and play with the tools if interested) before the tutorial on Feb 12.*

## Assignment Objective

In assignment 1, you are expected to: 
1. Generate test cases using Randoop (Task 1); 
2. Use EclEmma to measure test coverage (Task 1);
3. Build a test coverage tool with Soot (Tasks 2 & 3);
4. Explore the usefulness of GenAI in test preparation (Task 4).

*We have arranged a tutorial course to brief you on this assignment. Note that the tutorial may not have covered all required Soot APIs. You need to pick up some APIs by yourself.*

### Having Questions?
1. If you have questions, please check our [FAQ](Assignment1_FAQ.md). 
2. If your problem is not solved, you can create `Issues` in this repository.
 We will try to address the issues you have created within 24 hours. **You are highly encouraged to start early so you can accommodate unexpected uncertainties that arise from hardware and software.
3. If you don't want your question to be visible to other classmates, you can email the TA. 

## Assignment Materials

### Class Under Test

Please download/fork the project we prepared for this assignment in this repo.

The project contains a Java Class under test (CUT), `comp5111.assignment.cut.Subject`. The Java source file resides in `src/main/java/`.

You need to generate test suites for this program and measure their test coverages, i.e., the percentages of statements/branches/lines of this program covered by each generated test suite. 

This program includes a Java class with three inner classes, and you should work with the whole program, including all of the inner classes. 
That said, *all `inner classes` of CUT should be considered in the test*.

The CUT program we prepared is buggy. You do not need to locate the bugs in Assignment 1; the task of bug localization will be conducted in Assignment 2.

### Platform

Assignment 1 requires using Eclipse IDE and command lines with Java 11 (SE).
The following environment and library versions are recommended.

#### Environment
- Linux / MacOS / Windows
- Java 11 (SE)
- Eclipse 2024-12
  
#### Libraries
- [Randoop 4.3.1](https://github.com/randoop/randoop/releases/download/v4.3.1/randoop-4.3.1.zip)
- [Soot 4.2.1](https://repo1.maven.org/maven2/org/soot-oss/soot/4.2.1/soot-4.2.1-jar-with-dependencies.jar)
- [JUnit 4.12](https://repo1.maven.org/maven2/junit/junit/4.12/junit-4.12.jar), with [hamcrest-1.3](https://repo1.maven.org/maven2/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar) (only needed if you get errors)

If you use `maven`, these dependencies have already been declared in `pom.xml`. In case you want to use the `jar` file of the libraries directly, we have already included them in `lib/`.

## Assignment Tasks

### Task 1: Test case generation and Code coverage measurement (26%)

You are expected to finish 2 subtasks.

### Task 1.1: Learn how to use Randoop for unit test generation.

Randoop is a tool for JUnit test generation.
A short usage instruction for Randoop is available [here](./tutorials/randoop.md), including how to use Randoop to generate unit tests, and execute them using Eclipse.

#### Requirements: 
1. You need to use Randoop to generate **5 different test suites** for the given program.
2. Each test suite must achieve **at least 50% line coverage**.
3. The 5 test suites should be different from each other. Randoop parameter `--randomseed` may be helpful.
4. The package name of generated test suites should be `comp5111.assignment.cut`. Randoop parameter `--junit-package-name` may be helpful.
5. The 5 test suites should be put in `src/test/randoop<i>` for the `<i>`th test suite, where `0<=i<=4`. Randoop parameter `--junit-output-dir` may be helpful.

#### Submissions: 
Submit the 5 test suites generated by you, i.e., the folders `randoop0`, `randoop1`, `randoop2`, `randoop3`, and `randoop4` in `src/test`. 

#### Grading Scheme:
Each test suite accounts for `2%` if it successfully achieves at least 50% line coverage.

### Task 1.2: To learn how to use EclEmma for code coverage measurement.

EclEmma is a bundled Eclipse plugin to measure code coverage. A usage instruction is available [here](./tutorials/eclipse_eclemma.md).

#### Requirements:
1. Please use EclEmma to measure the coverage of each of the 5 submitted test suites generated by Randoop.

#### Submissions:
For **each test suite**, please submit a screenshot showing the `line coverage`, and a screenshot for `branch coverage`. 
In total, you are expected to submit a folder containing **10 screenshots**.
Each screenshot should be properly named to identify the corresponding line coverage or branch coverage.

#### Grading Scheme: 
Each screenshot accounts for `1.6%` if the values in the screenshot are correct.


### Task 2: Statement coverage measurement using Soot (25%)

Soot is a framework for analyzing and transforming Java programs.
We prepared a short introduction about Soot [here](./tutorials/soot.md). Answers to some common questions are available [here](Assignment1_FAQ.md).

For Tasks 2 and 3, you may refer to an [example test suite](./src/test/example_test) along with its [statement coverage](./src/test/example_test/TA-stmt.txt) and [branch coverage](./src/test/example_test/TA-branch.txt) collected by our implementation using Soot. You can run your program on this example test suite and crosscheck the coverage results with ours. The coverage reported by your tool should be close to ours. 
Besides, we *strongly recommend* you **follow the style of our reports** to present the necessary information including the coverage of the whole classes and each statement/branch, the detailed numbers of covered and missed statements/branches, the corresponding line of code of each statement, etc.

#### Requirements:
1. Use Soot to instrument the given CUT to measure the statement coverage of tests.
 By statements, we refer to `Jimple statement`, which is an intermediate representation of the Java program in Soot.
2. Your program needs to generate a report file, which **at least contains the following information**:
   * The statement coverage for each class (final percentage and raw numbers of the covered and total statements), including the inner classes.
   * The coverage of each statement (statement code, yes or no).
   * The line number in the Java source file for each statement.
3. You should prepare a one-click script to automatically use your implemented Soot-based tool to report the statement coverage of the `example_test` test suite. You should also put down the usage instructions of your one-click scripts, together with the path and meaning of the output files.

#### Notes:
- When instrumenting the CUT, **you are required to skip the `JIdentityStmt`**. Please refer to [FAQ Q3](./Assignment1_FAQ.md#3-why-we-got-the-exception-javalangruntimeexception-param-assignment-statements-should-precede-all-non-identity-statements-) on why we need to skip the `JIdentityStmt` and the hint on how to skip it.
- To validate the correctness of your statement coverage report, you can crosscheck it with the report generated by EclEmma. The coverage reported by your tool should be **close to** that reported by EclEmma for the classes in the CUT. 
  - *Note it is normal that the number of covered/total statements reported by Soot and EclEmma may not be the same since Soot and EclEmma consider different types of statements, and thereby, there could be a small difference (typically within 5%) in their reported statement coverage.*

#### Submissions:
1. You may put your source code under `comp5111.assignment` package in `src/main/java` folder.
2. You are recommended to use Java build tools to manage your project, e.g., maven, or gradle.
3. **Please prepare useful scripts and readme files in the project root**, containing the one-click script and instructions on running your implementation as mentioned in the requirements.
4. You need to measure the coverage of the example test suite.
5. A file showing the differences in the percentage (statement coverage for all inner classes) reported by your tool and EclEmma. *Note that: minor differences are allowed for statement coverage.*
6. Advice: You are highly encouraged to maintain your code in a `private` repository at Github. This provides a backup of your code in case your computer crashes and allows us to retrieve code from your GitHub repository in case of submission errors. Such incidents have happened in the past.

#### Grading Scheme:
- `15%` if your soot-based program can successfully instrument the code and report the coverage following the requirements.
- `10%` if the results reported by your code are correct on the hidden test cases from the teaching team.


### Task 3: Branch coverage measurement using Soot (25%)

#### Requirements:
1. Use Soot to instrument the given CUT to measure `branch coverage` of tests.
2. Your program needs to generate a report file, which **at least contains the following info**:
   * The branch coverage for each class (final percentage and raw numbers of the covered and total branches), **including the inner classes**.
   * The coverage of each branch (branch code, yes or no).
3. You should also prepare a one-click script and usage instructions for Task 3, as in Task 2.

#### Notes:
- You may reuse some of your code in task 2. We consider **each statement** in task 2, while we consider **each branch** in task 3.
- The branches for Soot and EclEmma are similar, so the difference in branch coverage they reported should be small.

#### Submissions:
(Same to Task 2.)

#### Grading Scheme:
- `15%` if your program can successfully instrument the program and report the coverage.
- `10%` if the results reported by your code are correct on the hidden test cases from the teaching team.


### Task 4: Exploring the usefulness of Generative AI in test preparation (24% + 3% bonus)

#### Requirements:
1. Explore the usefulness of large language models (LLMs) like GPT4o-mini, Gemini-flash, QWen, and DeepSeek in two test preparation tasks.
2. Compare the usefulness of a relatively larger (and potentially stronger) LLM and a handy and smaller LLM. The larger LLM typically includes *over 15B* parameters, while the smaller should include *fewer than 10B* parameters. For the commercial LLMs whose parameter volume may be unavailable from public documents, it is fine to consider the much more powerful and expensive versions like GPT-4o and Gemini-Pro as the larger LLM while the cheaper versions like GPT-4o-mini and Gemini-Flash as the smaller LLM. Besides, it is recommended to consider a pair of models from the same family, for example, GPT4o & GPT4o-mini, Gemini-Pro & Gemini-Flash, Qwen2.5-Coder-32B & 7B, etc.
3. Write a study report on the effectiveness of two LLMs at different sizes on two test preparation tasks and suggest methods to enhance the performance of smaller LLMs.

#### Tasks about LLMs for Test Preparation

1. **Test Generation:** Design a prompt for LLMs to generate test cases for some (no need for all) methods in `Subject.java`. The performance of LLM can be evaluated based on the proportions of compilable tests among all tests generated by the LLM, the proportion of tests with correct assertions, and the proportion of fault-triggering tests, etc.
2. **Coverage Enhancement:** Design a prompt for LLMs to enhance the branch coverage of some tests generated by Randoop in Task 1, for some (no need for all) methods in `Subject.java`. The performance of LLM can be demonstrated based on the coverage difference between the original tests generated by Randoop and the ones improved by LLMs.

#### Notes:
- For each of the two tasks above, you may explore various prompts and record the **best prompt** for the models. You should answer the following questions for each of the tasks:
  1. Put down the names of the larger and smaller LLMs you choose.
  2. Explore an effective prompt to drive **the larger LLM** to realize the goal effectively. The prompt can be based on in-context learning, including few-shot examples and a chain of thoughts.
  3. Analyze the performance of **the smaller LLM** under the same prompt. Summarize and demonstrate 3~5 typical limitations and failure phenomena of the smaller LLM compared to the larger LLM.
  4. Suggest an enhanced prompting strategy (e.g., few-shot learning and chain-of-thought) to improve the performance of **the smaller LLM** against the identified limitations and validate its effectiveness.
  5. If the smaller LLM is as capable as the larger one, you may skip questions 2 and 3. Instead, summarize and demonstrate 5~8 comparable key qualities (e.g., rate of syntax-correct tests, correct assertions, coverage, readability, etc.) that the smaller LLM shares with the larger LLM.
- Some potentially helpful links to access LLMs: https://genai.ust.hk/, https://poe.com/, https://lmarena.ai/.
- There should be (around) 300-600 illustrative words (not counting codes) and appropriate screenshots in your answer to each of the questions above, and you should answer these questions for each of the two test preparation tasks. Try to make your report informative yet concise.
- Baseline Answer Framework (for reference):
```text
Larger LLM: GPT-4o on Poe
Smaller LLM: GPT-4o-mini on Poe

# [Task 1: Test Generation]

## [Question 1: Effective prompt for GPT-4o]

### Effective Prompt Strategy: 

I use a Chain-of-Thought prompt to drive GPT-4o to prepare tests.

Prompt:
I: Generate five diverse failing unit tests with assert statements for the following function: public int getIndex(final String exp) {<omit code>}. You may draft a preliminary version, and then explain and validate the assertion in your generated tests. Revise it if you find it not good.
LLM: xxxxxx.

### Analysis of the effectiveness:
Using this prompt, GPT-4o can effectively generate tests. Specifically, it can effectively analyze the intention of ABC based on the information about P and Q given in the prompt (explanation)...
The 3 tests generated by GPT-4o are XXX, YYY, ZZZ.
Compilable tests: XXX, YYY. (The issue of ZZZ is ...)
Test with a correct assertion: XXX. (The issue of YYY is ...)

## [Question 2: Comparison between GPT-4o and GPT-4o-mini]

### Performance of GPT-4o-mini:

Using this prompt, GPT-4o-mini's success rate in generating high-quality tests decreases. Specifically, it can only generate xx out of XX compilable tests. yy out of YY tests are with reliable assertions...

### Outstanding Limitations of GPT-4o-mini:

I found GPT-4o-mini does much poorer than GPT-4o in these perspectives:
1. Syntax correctness of generated tests. Specifically, GPT-4o-mini often misses ... and thereby ... For example, in case 1, ...; in case 2, ...
2. ...
...(omit)...

## [Question 3: Enhanced Prompting Strategy for GPT-4o-mini]

I use a Chain-of-Thought approach, along with few-shot prompting and self-reflection techniques, to drive GPT-4o-mini and enhance the syntax correctness of generated tests.

Prompt:
I: ...(few-shot examples)... You are... 
LLM: xxxxxx.

Few-shot examples are static and prepared by me. I chose xx, yy, and zz as the static examples since they are comprehensive... / dynamic, and how to select them and why they are helpful ...

It is found that with the enhanced few-shot prompting and self-reflection techniques, GPT-4o-mini can generate more syntaxically correct tests. Specifically, (some metrics, some comparisons, some case studies, etc.) ...

# [Task 2: Coverage Enhancement]

...(omit)...
```

#### Submissions:
Submit a `pdf` file as your report, following the content format illustrated by the above example. 
Name the report as `Task4_Report.pdf` and put it under the root folder of your submission.

#### Grading Scheme:
- `4%` for a good prompt for the larger model in each task, `8%` in total. Scores will be earned if the prompt is reasonable and demonstrated to be effective in driving the larger LLM to work out the corresponding task.
- `4%` for an extensive comparison between the larger and smaller LLMs in each task, `8%` in total. Scores will be earned if the justification clearly demonstrates the performance differences between the larger and smaller LLMs in the according task.
- `4%` for the quality of the suggested strategy for enhancing the smaller LLM, `8%` in total. Scores will be earned if the justification clearly demonstrates that the limitations are effectively alleviated after adopting the suggested strategy for the smaller LLM.
- `+3%` *bonus* if an LLM with no more than 3B parameters (e.g., Qwen2.5-Coder-1.5B and Granite3.1-2b) is chosen as the smaller model to analyze and its performance is effectively enhanced.

---

## Assignment 1 Submission

- You are required to submit your assignment to [Canvas](https://canvas.ust.hk/courses/61173/assignments/).
- Please put all your code, screenshots, readme, and so on into a single folder and compress it to `comp5111asn1-YourStudentID-YourLastName-YourFirstName.zip`

**The *recommended* folder structure is:**

1. Put your code into `${PROJECT_ROOT}/src/main/java/`
2. If you do not use Java build tools, put the libraries jar files that your code depends on into `${PROJECT_ROOT}/lib/`
3. Put your running scripts under `${PROJECT_ROOT}/scripts`
4. Put your screenshot into `${PROJECT_ROOT}/screenshots`
5. Put the test suites generated by you into `${PROJECT_ROOT}/src/test/randoop[0-4]`
6. Put the report at `${PROJECT_ROOT}/Task4_Report.pdf`.
